"""
BlogArchiveParser - ë„¤ì´ë²„ ë¸”ë¡œê·¸ í…ìŠ¤íŠ¸ íŒŒì¼ íŒŒì„œ

ì‚¬ì§„ ê°œìˆ˜N êµ¬ë¶„ìž ê¸°ë°˜ ìƒíƒœ ë¨¸ì‹ ìœ¼ë¡œ í¬ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.
"""

import re
import uuid
from datetime import date, datetime
from pathlib import Path
from typing import List, Optional, Tuple

from src.shared.models import BlogArchive


class BlogArchiveParser:
    """ë¸”ë¡œê·¸ ì•„ì¹´ì´ë¸Œ í…ìŠ¤íŠ¸ íŒŒì¼ íŒŒì„œ"""

    PHOTO_COUNT_RE = re.compile(r"^ì‚¬ì§„ ê°œìˆ˜(\d+)$")
    DATE_RE = re.compile(r"^(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})\.$")
    SEO_MEMO_MARKER = "\U0001f4c8"  # ðŸ“ˆ

    class _State:
        IDLE = "idle"
        AWAITING_TITLE = "awaiting_title"
        AWAITING_CONTENT = "awaiting_content"
        AWAITING_DATE = "awaiting_date"
        AWAITING_VIEWS = "awaiting_views"

    def __init__(self, source_file: str = "blog-cctv.txt"):
        self.source_file = source_file

    def parse_file(self, file_path: str) -> List[BlogArchive]:
        """íŒŒì¼ ì „ì²´ë¥¼ íŒŒì‹±í•˜ì—¬ BlogArchive ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        path = Path(file_path)
        if not path.exists():
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")

        with open(path, "r", encoding="utf-8") as f:
            lines = f.readlines()

        lines = [line.rstrip("\n") for line in lines]
        return self._parse_lines(lines)

    def _parse_lines(self, lines: List[str]) -> List[BlogArchive]:
        """ë¼ì¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒíƒœ ë¨¸ì‹ ìœ¼ë¡œ íŒŒì‹±"""
        posts: List[BlogArchive] = []
        state = self._State.IDLE
        parse_order = 0

        photo_count = 0
        title = ""
        content = ""
        start_line = 0
        original_date = date(2025, 1, 1)

        for line_num, raw_line in enumerate(lines, start=1):
            stripped = raw_line.strip()

            if state == self._State.IDLE:
                match = self.PHOTO_COUNT_RE.match(stripped)
                if match:
                    photo_count = int(match.group(1))
                    start_line = line_num
                    state = self._State.AWAITING_TITLE

            elif state == self._State.AWAITING_TITLE:
                if stripped:
                    title = stripped
                    state = self._State.AWAITING_CONTENT

            elif state == self._State.AWAITING_CONTENT:
                # ë¹ˆ ì¤„ ê±´ë„ˆë›°ê³  ì²« ë²ˆì§¸ ë¹„ì–´ìžˆì§€ ì•Šì€ ì¤„ = ë³¸ë¬¸
                if stripped:
                    content = stripped
                    state = self._State.AWAITING_DATE

            elif state == self._State.AWAITING_DATE:
                date_match = self.DATE_RE.match(stripped)
                if date_match:
                    try:
                        original_date = date(
                            int(date_match.group(1)),
                            int(date_match.group(2)),
                            int(date_match.group(3)),
                        )
                    except ValueError:
                        original_date = date(2025, 1, 1)
                    state = self._State.AWAITING_VIEWS
                # ë‚ ì§œê°€ ì•„ë‹Œ ì¤„ì€ ë³¸ë¬¸ì˜ ì—°ì†ìœ¼ë¡œ ì²˜ë¦¬ (í˜¹ì‹œ ë©€í‹°ë¼ì¸ì¸ ê²½ìš°)
                elif stripped:
                    content += " " + stripped

            elif state == self._State.AWAITING_VIEWS:
                if stripped.isdigit():
                    view_count = int(stripped)
                elif stripped == "":
                    # ë¹ˆ ì¤„ì´ë©´ ì•„ì§ ì¡°íšŒìˆ˜ ëŒ€ê¸°
                    continue
                else:
                    # ì¡°íšŒìˆ˜ê°€ ì•„ë‹Œ ë‹¤ë¥¸ ë‚´ìš© â†’ ì¡°íšŒìˆ˜ 0ìœ¼ë¡œ ì²˜ë¦¬
                    view_count = 0

                # SEO ë©”ëª¨ ë¶„ë¦¬
                seo_memo, clean_content = self._extract_seo_memo(content, title)

                parse_order += 1
                post = BlogArchive(
                    id=str(uuid.uuid4()),
                    original_title=title,
                    original_content=content,
                    seo_memo=seo_memo,
                    clean_content=clean_content,
                    photo_count=photo_count,
                    original_date=original_date,
                    view_count=view_count,
                    word_count=len(clean_content),
                    has_seo_memo=seo_memo is not None,
                    source_file=self.source_file,
                    source_line=start_line,
                    parse_order=parse_order,
                    created_at=datetime.now(),
                    updated_at=datetime.now(),
                )
                posts.append(post)

                # ìƒíƒœ ë¦¬ì…‹
                state = self._State.IDLE
                photo_count = 0
                title = ""
                content = ""
                start_line = 0

                # í˜„ìž¬ ì¤„ì´ ë‹¤ìŒ ë¸”ë¡ì˜ ì‹œìž‘ì¸ì§€ í™•ì¸
                if not stripped.isdigit():
                    next_match = self.PHOTO_COUNT_RE.match(stripped)
                    if next_match:
                        photo_count = int(next_match.group(1))
                        start_line = line_num
                        state = self._State.AWAITING_TITLE

        return posts

    def _extract_seo_memo(
        self, content: str, title: str
    ) -> Tuple[Optional[str], str]:
        """SEO ë©”ëª¨(ðŸ“ˆ ë¸”ë¡)ë¥¼ ë³¸ë¬¸ì—ì„œ ë¶„ë¦¬

        SEO ë©”ëª¨ê°€ ìžˆëŠ” í¬ìŠ¤íŠ¸ëŠ” ë³¸ë¬¸ì´ ðŸ“ˆë¡œ ì‹œìž‘í•˜ë©°,
        ì‹¤ì œ ì œëª©ì´ ë³¸ë¬¸ ë‚´ì—ì„œ ë°˜ë³µë˜ëŠ” ì§€ì ê¹Œì§€ê°€ ë©”ëª¨ìž…ë‹ˆë‹¤.
        """
        if not content.startswith(self.SEO_MEMO_MARKER):
            return None, content

        # ì œëª©ì´ ë³¸ë¬¸ì—ì„œ ë°˜ë³µë˜ëŠ” ìœ„ì¹˜ ì°¾ê¸°
        title_idx = content.find(title)
        if title_idx > 0:
            seo_memo = content[:title_idx].strip()
            clean_content = content[title_idx:].strip()
            return seo_memo, clean_content

        # ì œëª©ì„ ëª» ì°¾ì€ ê²½ìš° ì „ì²´ë¥¼ cleanìœ¼ë¡œ ë°˜í™˜
        return content, ""

    def get_stats(self, posts: List[BlogArchive]) -> dict:
        """íŒŒì‹± ê²°ê³¼ í†µê³„ ë°˜í™˜"""
        if not posts:
            return {"total": 0}

        dates = [p.original_date for p in posts if p.original_date]
        seo_count = sum(1 for p in posts if p.has_seo_memo)

        return {
            "total": len(posts),
            "with_seo_memo": seo_count,
            "without_seo_memo": len(posts) - seo_count,
            "date_range": {
                "earliest": min(dates).isoformat() if dates else None,
                "latest": max(dates).isoformat() if dates else None,
            },
            "avg_word_count": sum(p.word_count for p in posts) // len(posts),
            "total_views": sum(p.view_count for p in posts),
        }
