# 2026년 2월 기준 브라우저 자동화용 VLM 비용-성능 분석

## BLUF (핵심 결론 먼저)

네이버 블로그 발행 에이전트 (월 ~4,000회 VLM 호출, 예산 $10 이하) 기준으로 **3가지 최적 선택지**:

1. **Qwen3-VL-Flash** — 월 ~$1.40, 한국어 OCR 우수, 구조화 JSON 출력 안정
2. **Gemini 2.0 Flash-Lite** — 월 ~$1.65, Google 인프라 안정성
3. **로컬 배포 UI-TARS-7B / Qwen2.5-VL-7B** — API 비용 $0, GUI 자동화 특화

**기존 플랜의 DeepSeek VL2는 문제가 있다** — DeepSeek 공식 API에 멀티모달 엔드포인트가 없음. Replicate 호스팅(~$5.60/월) 또는 80GB+ VRAM 셀프호스팅만 가능.

가장 충격적인 발견: **UI 특화 모델(UI-TARS)이 범용 VLM 대비 77배 성능 차이**. GPT-4o가 ScreenSpot-Pro에서 0.8%인 반면, UI-TARS-1.5-7B는 61.6% 달성.

---

## 1. DeepSeek VL2 현실 점검: 공식 멀티모달 API 없음

현재 플랜의 가장 큰 문제점이다.

**DeepSeek 공식 API는 텍스트 모델만 지원**한다 (deepseek-chat, deepseek-reasoner). VL2 비전 모델의 직접 클라우드 API 엔드포인트는 존재하지 않는다.

> 📖 **Replicate 호스팅**
> - **정의**: 오픈소스 모델을 클라우드 GPU에서 API로 서빙해주는 플랫폼
> - **쉬운 비유**: "내 모델을 남의 컴퓨터에 올려서 API처럼 쓰는 것"
> - **비용**: ~$0.0014/회 → 4,000회 = **~$5.60/월**
> - **문제**: latency 1-3초, cold start 추가 지연

DeepSeek VL2는 MoE(Mixture-of-Experts) 아키텍처로 3가지 변형이 있다:
- VL2-Tiny: 1B 활성 파라미터
- VL2-Small: 2.4B 활성 파라미터
- VL2 (풀): 27B 중 4.5B 활성 → **셀프호스팅에 80GB+ VRAM 필요**

MIT 라이선스로 모델 자체는 훌륭하지만, 직접 API가 있는 경쟁 모델 대비 **인프라 오버헤드가 너무 크다**.

---

## 2. 클라우드 API 가격 비교 (월 4,000회 호출 기준)

**산출 기준**: 호출당 1080p 스크린샷(~1,000-1,500 토큰) + DOM 텍스트 2,000 토큰 입력, ~500 토큰 응답

| 모델 | 입력 ($/1M) | 출력 ($/1M) | **월 비용** | 응답속도 | JSON 지원 |
|------|------------|------------|-----------|---------|----------|
| **🥇 Qwen3-VL-Flash** | $0.05 | $0.40 | **$1.40** | 700-2000ms | ✅ 우수 |
| **🥈 Gemini 2.0 Flash-Lite** | $0.075 | $0.30 | **$1.65** | 600-2000ms | ✅ 네이티브 |
| GPT-4.1-nano | $0.10 | $0.40 | **$2.04** | 500-1500ms | ✅ 네이티브 |
| Gemini 2.0 Flash | $0.10 | $0.40 | **$2.20** | 600-2000ms | ✅ 네이티브 |
| GPT-4o-mini | $0.15 | $0.60 | **$3.06** | 500-1500ms | ✅ 네이티브 |
| DeepSeek VL2 (Replicate) | ~$0.0014/회 | — | **$5.60** | 1000-3000ms | ⚠️ 불안정 |
| GPT-4.1-mini | $0.40 | $1.60 | **$8.17** | 500-1500ms | ✅ 네이티브 |
| Claude 3.5 Haiku | $0.80 | $4.00 | **$18.56** | 800-2500ms | ✅ 네이티브 |

**Qwen3-VL-Flash가 압도적 1위** — GPT-4o-mini 대비 3배 저렴하면서 한국어 포함 다국어 OCR이 강력하다. 알리바바 클라우드 API는 90일간 100만 토큰 무료 쿼터 + 배치 처리 50% 할인도 제공.

참고: OpenAI의 **low-detail 모드**를 쓰면 이미지 토큰이 ~1,105 → 85로 90%+ 감소하여 GPT-4.1-nano가 ~$1.17/월까지 내려가지만, 한국어 UI 텍스트 인식 정확도가 크게 떨어진다.

---

## 3. UI 특화 모델: 게임 체인저

이번 리서치에서 가장 중요한 발견이다. **GUI 전용 모델이 범용 VLM을 압도적으로 능가한다.**

> 📖 **ScreenSpot / ScreenSpot-Pro**
> - **정의**: GUI 요소의 위치를 정확히 찾아내는 능력(grounding)을 측정하는 벤치마크
> - **쉬운 비유**: "스크린샷을 보고 '발행 버튼'이 어디에 있는지 정확히 짚어내는 시험"
> - **Pro 버전**: 전문 소프트웨어(IDE, CAD 등)의 복잡한 UI까지 포함
> - **왜 중요한가**: 브라우저 자동화에서 "어디를 클릭할지"가 핵심이므로 이 벤치마크가 직접적 성능 지표

### UI-TARS (ByteDance)
- Qwen2.5-VL 기반 end-to-end GUI 에이전트
- **ScreenSpot-Pro 61.6%** (GPT-4o는 0.8% — 77배 차이)
- 웹/데스크톱/모바일/게임 크로스플랫폼 지원
- Midscene.js와 직접 통합 가능 (JavaScript 브라우저 자동화)
- 7B 버전: **24GB GPU에 8-bit 양자화로 배포 가능**
- Apache 2.0 라이선스

### ShowUI (NUS + Microsoft)
- **2B 파라미터**로 경량 — ScreenSpot 75.1% (zero-shot)
- 비주얼 토큰 33% 절감 → 빠른 추론
- Gradio 기반 API로 로컬 GPU 없이도 테스트 가능

### OmniParser (Microsoft)
- 스크린샷 → 구조화된 요소 목록으로 변환하는 **전처리 도구**
- GPT-4o와 결합 시 ScreenSpot-Pro: 0.8% → **39.6%** (50배 향상)
- RTX 4090에서 0.8초/프레임

> 📖 **OmniParser의 역할**
> - **쉬운 비유**: VLM이 "안경"을 쓰는 것 — 스크린샷의 버튼, 입력칸, 링크를 미리 표시해줌
> - **메커니즘**: Screenshot → OmniParser(요소 감지) → 저가 VLM(행동 결정) → 실행
> - **존재 이유**: 싸고 작은 모델도 "어디를 클릭할지" 정확히 판단 가능하게 함

| 모델 | ScreenSpot | ScreenSpot-Pro | 파라미터 | 배포 |
|------|-----------|----------------|---------|------|
| **UI-TARS-1.5-7B** | ~90% | **61.6%** | 7B | Apache 2.0, 로컬/API |
| ShowUI-2B | **75.1%** | — | 2B | 오픈소스, HF API |
| CogAgent-9B | 상위권 | 상위권 | 9B | 오픈소스 |
| GPT-4o (기준선) | ~70% | 0.8% | — | API 전용 |
| GPT-4o + OmniParser | — | **39.6%** | — | 하이브리드 |

---

## 4. 로컬 배포 경제성

월 4,000회 추론 = ~2.2시간 컴퓨팅(호출당 2초 기준). RunPod/Vast.ai 클라우드 GPU 대여 시 **월 $1-2** — 대부분의 API 옵션보다 저렴.

| 모델 | FP16 VRAM | 4-bit VRAM | 추론 속도 | 한국어 |
|------|-----------|------------|----------|--------|
| Moondream 2B | 5.2GB | 2.5GB | 40+ tok/s | ⚠️ 제한적 |
| **Qwen2.5-VL-7B** | 15-17GB | 4.5GB | 25-35 tok/s | **✅ 우수** |
| **UI-TARS-7B** | 24GB | ~12GB | 20-30 tok/s | ✅ 양호 |
| InternVL2.5-8B | 18GB | 6GB | 20-30 tok/s | ✅ 양호 |

**Qwen2.5-VL-7B**: 한국어 OCR 학습 데이터에 명시적으로 포함. AWQ 4-bit 양자화 시 4.5GB까지 축소 가능. vLLM(최고 처리량), Ollama(가장 쉬운 설정), LMDeploy 지원.

**UI-TARS-7B**: GUI 에이전트로 설계되어 별도 perception/planning 분리 불필요. 8-bit 양자화로 ~16GB VRAM.

**Moondream 2B**: 초경량 옵션. ~5GB VRAM에 40+ tok/s. UI 요소 위치 감지 내장. 단순 스텝에 적합하지만 한국어 지원 약함.

---

## 5. 한국어 지원: 모델별 격차가 크다

네이버 블로그 자동화에서 **한국어 텍스트 인식 품질이 신뢰도를 직접 결정**한다.

### 한국어 강자
- **Qwen2.5-VL**: 29개 이상 언어 OCR 학습, 특히 CJK(한중일) 강세. MTVQA(다국어 텍스트 VQA) 벤치마크 SOTA
- **네이버 HyperCLOVA X Vision**: 한국어 데이터 GPT-4 대비 6,500배 학습. 한국 검정고시 83.8% (GPT-4o: 77.8%). 오픈소스 3B 버전 존재 (HyperCLOVAX-SEED-Vision-Instruct-3B)
- **GPT-4o 계열**: o200k_base 토크나이저로 한국어 커버리지 2배 향상

### 한국어 약자 (피해야 할 모델)
- **Llama 3.2 Vision**: 벤치마크 테스트에서 CJK 문자 구분 불가 확인
- **Claude**: 전반적으로 유능하지만 CJK 비전 태스크에서 Qwen/GPT-4o 대비 약함

### 네이버 특이사항
⚠️ 2025년 7월 기준, **네이버 블로그가 Selenium 기반 자동화를 적극 차단** 중. 행동 패턴 감지 정교화. **Playwright 필수**, 스크롤/클릭 타이밍에 주의 필요.

---

## 6. 브라우저 자동화 프레임워크들의 VLM 선택

### Browser-Use (가장 인기 있는 오픈소스)
- **ChatBrowserUse** 자체 모델 출시 — "타 모델 대비 3-5배 빠르고 SOTA 정확도"
- 2025년 12월 BU-30B-A3B-Preview: "200 태스크/$1" (4배 비용 효율)
- 외부 모델 추천: OpenAI O3 (최고 정확도)
- ⚠️ Alibaba 모델 중 Qwen-VL-Max만 안정 작동 (나머지는 스키마 문제)

### Skyvern — 가장 혁신적 비용 최적화

> 📖 **Compile-to-Code 접근법**
> - **정의**: AI가 워크플로우를 1번 탐색(비쌈) → 성공 경로를 Playwright 스크립트로 컴파일 → 이후 저렴하게 재생
> - **쉬운 비유**: 내비게이션으로 한 번 가본 길을 기억해서, 다음부턴 내비 없이 가는 것
> - **핵심**: 반복적 태스크(블로그 발행!)에서 VLM 비용을 극적으로 절감
> - **VLM 재개입 시점**: 사이트 변경으로 스크립트가 깨질 때만

**이 방식이 네이버 블로그 발행에 가장 적합한 패턴이다** — 발행 워크플로우는 대부분 고정적이므로.

### LaVague
- RAG로 관련 HTML 청크만 선별 검색 → 토큰 소비 절감
- 기본 GPT-4o, Ollama 통한 로컬 모델 지원

---

## 7. 하이브리드 아키텍처 제안

모든 스텝에 동일 모델을 쓸 필요가 없다. 복잡도별 3단계 접근:

**Tier 1 (단순 네비게이션)**: Moondream 2B 로컬 or Gemini 2.0 Flash-Lite API
- "X 버튼 클릭", 기본 요소 식별
- 최저 비용

**Tier 2 (표준 스텝)**: Qwen2.5-VL-7B 로컬 or Qwen3-VL-Flash API
- 폼 입력, 드롭다운 선택, 대부분의 발행 스텝
- 한국어 UI 인식

**Tier 3 (복잡한 판단)**: GPT-4.1-mini or Claude
- CAPTCHA, 비정상 레이아웃, 에러 복구
- 고급 추론 필요 시만 호출

**OmniParser 전처리**: +0.8초 latency지만 저가/소형 VLM의 grounding 정확도를 극적으로 향상.

```
스크린샷 → OmniParser(요소 감지) → 저가 VLM(행동 결정) → 실행
```

---

## 8. 최종 권장안

### 🏆 클라우드 API 추천: Qwen3-VL-Flash
- 월 **~$1.40** (예산의 14%)
- 한국어 OCR 우수 + 구조화 JSON 안정
- 알리바바 클라우드 API (90일 무료 쿼터)
- **백업**: Gemini 2.0 Flash-Lite (~$1.65/월)

### 🏆 로컬 배포 추천: UI-TARS-7B 또는 Qwen2.5-VL-7B
- 24GB GPU (RTX 4090/3090)에서 구동
- API 비용 $0
- UI-TARS는 GUI 자동화 특화 → think/act 루프에 최적
- Qwen2.5-VL은 한국어 OCR 최강

### ❌ DeepSeek VL2 — 기존 플랜 수정 필요
- 공식 멀티모달 API 없음
- Replicate 호스팅 시 ~$5.60/월 + 1-3초 latency
- 동일 예산으로 Qwen3-VL-Flash가 4배 저렴하고 더 빠름

### 💡 핵심 최적화 전략
1. **Skyvern 스타일 경로 기억**: 안정적 워크플로우 스텝은 1회 VLM 탐색 → Playwright 스크립트 컴파일 → 이후 VLM 호출 없이 재생
2. **OmniParser 전처리**: 저가 모델의 grounding 정확도 50배 향상
3. **Playwright 사용 필수**: 네이버의 Selenium 차단 대응
4. **하이브리드 tiering**: 단순 스텝은 저가 모델, 복잡한 판단만 고가 모델

---

## 기존 플랜 영향도

| 플랜 항목 | 변경 필요 여부 |
|----------|--------------|
| agent/base_agent.py (think→act 루프) | ✅ 유지 — 모델 무관 |
| tools/browser_tool.py (CDP 액션) | ✅ 유지 — 모델 무관 |
| **llm/vision_client.py** | ⚠️ **수정 필요** — DeepSeek → Qwen3-VL-Flash API 또는 UI-TARS 로컬 |
| agent/prompts.py | ✅ 유지 — 프롬프트 구조 동일 |
| schema.py | ✅ 유지 — 모델 무관 |
| adaptive_publisher.py (래퍼) | ✅ 유지 — 모델 무관 |

**결론**: 플랜의 아키텍처(OpenManus 스타일 에이전트 루프)는 그대로 유효. **VLM 클라이언트 레이어만 교체하면 된다.** `DeepSeekVisionLLM` → `QwenVisionLLM` 또는 `UITarsLocalLLM`으로 변경.
